---
title: "OneSite_annual_cQ_Analysis"
author: "Marc Peipoch"
date: '2023-01-12'
output: html_document
---

Input data files and format the into requried cQ format
```{r setup, include=FALSE}
library(zoo) ; library(tidyr) ; library(lubridate) ; library(dplyr)

site="ARIK"


setwd(paste("R:/EcosystemEcologyLab/MSAPLANKTONdataDirectory/DataInDevelopment/NEONData/",site,"/03_incremental",sep=""))


merged_data = read.csv("final_data_30min_2017.csv", header = T)

#bring in datetime vector
dateseq = read.csv("dates_2017.csv", header = T)
dateseq$datetime = lubridate::as_datetime(dateseq$datetime, tz = "UTC")
dateseq$datetime = format(dateseq$datetime,format='%m/%d/%Y %H:%M')

#######################################################################################################
merged_chldata_subset = merged_data[,c("datetime","meanFlow","chlorophyll")] ; 
merged_chldata_subset$meanFlow = merged_chldata_subset$meanFlow/1000
colnames(merged_chldata_subset) = c("datetime","q_cms","conc")
merged_chldata_subset = merged_chldata_subset[!duplicated(merged_chldata_subset[,c('datetime')]),] #remove duplicated rows by datetime
merged_chldata_subset$datetime = lubridate::as_datetime(merged_chldata_subset$datetime, tz = "UTC")
merged_chldata_subset$datetime = format(merged_chldata_subset$datetime,format='%m/%d/%Y %H:%M')

#merge with available data
merged_chldata_subset2 = merge.data.frame(dateseq,merged_chldata_subset,by="datetime",all.x = T)


#replace NAs with linear interpolation 
merged_chldata_subset2[1,2] = 0
merged_chldata_subset2[nrow(merged_chldata_subset2),2] = 0

merged_chldata_subset2$q_cms = na.approx(merged_chldata_subset2$q_cms)


merged_chldata_subset2[1,3] = 0
merged_chldata_subset2[nrow(merged_chldata_subset2),3] = 0

merged_chldata_subset2$conc = na.approx(merged_chldata_subset2$conc)

merged_chldata_subset2 = as.matrix(merged_chldata_subset2) 
colnames(merged_chldata_subset2) = c("datetime","q_cms","conc") ; merged_chldata_subset2 = as.data.frame(merged_chldata_subset2) 
merged_chldata_subset2$q_cms = round(as.numeric(merged_chldata_subset2$q_cms), digits = 4)
merged_chldata_subset2$conc = round(as.numeric(merged_chldata_subset2$conc), digits = 3)







merged_turbdata_subset = merged_data[,c("datetime","meanFlow","turbidity")] ; 
merged_turbdata_subset$meanFlow = merged_turbdata_subset$meanFlow/1000
colnames(merged_turbdata_subset) = c("datetime","q_cms","conc")
merged_turbdata_subset = merged_turbdata_subset[!duplicated(merged_turbdata_subset[,c('datetime')]),] #remove duplicated rows by datetime
merged_turbdata_subset$datetime = lubridate::as_datetime(merged_turbdata_subset$datetime, tz = "UTC")
merged_turbdata_subset$datetime = format(merged_turbdata_subset$datetime,format='%m/%d/%Y %H:%M')

#merge with available data
merged_turbdata_subset2 = merge.data.frame(dateseq,merged_turbdata_subset,by="datetime",all.x = T)

#replace NAs with linear interpolation 
merged_turbdata_subset2[1,2] = 0
merged_turbdata_subset2[nrow(merged_turbdata_subset2),2] = 0

merged_turbdata_subset2$q_cms = na.approx(merged_turbdata_subset2$q_cms)


merged_turbdata_subset2[1,3] = 0
merged_turbdata_subset2[nrow(merged_turbdata_subset2),3] = 0

merged_turbdata_subset2$conc = na.approx(merged_turbdata_subset2$conc)


merged_turbdata_subset2 = as.matrix(merged_turbdata_subset2) 
colnames(merged_turbdata_subset2) = c("datetime","q_cms","conc") ; merged_turbdata_subset2 = as.data.frame(merged_turbdata_subset2) 
merged_turbdata_subset2$q_cms = round(as.numeric(merged_turbdata_subset2$q_cms), digits = 4)
merged_turbdata_subset2$conc = round(as.numeric(merged_turbdata_subset2$conc), digits = 3)

#############
write.csv(merged_chldata_subset2,
          paste("R:/EcosystemEcologyLab/MSAPLANKTONdataDirectory/DataInDevelopment/NEONData/",site,"/03_incremental/merged_chldata_subset_2017.csv",sep="") , row.names=F)

write.csv(merged_turbdata_subset2, 
          paste("R:/EcosystemEcologyLab/MSAPLANKTONdataDirectory/DataInDevelopment/NEONData/",site,"/03_incremental/merged_turbdata_subset_2017.csv",sep="") , 
          row.names=F)





merged_data = read.csv("final_data_30min_2018.csv", header = T)

#bring in datetime vector
dateseq = read.csv("dates_2018.csv", header = T)
dateseq$datetime = lubridate::as_datetime(dateseq$datetime, tz = "UTC")
dateseq$datetime = format(dateseq$datetime,format='%m/%d/%Y %H:%M')

#######################################################################################################
merged_chldata_subset = merged_data[,c("datetime","meanFlow","chlorophyll")] ; 
merged_chldata_subset$meanFlow = merged_chldata_subset$meanFlow/1000
colnames(merged_chldata_subset) = c("datetime","q_cms","conc")
merged_chldata_subset = merged_chldata_subset[!duplicated(merged_chldata_subset[,c('datetime')]),] #remove duplicated rows by datetime
merged_chldata_subset$datetime = lubridate::as_datetime(merged_chldata_subset$datetime, tz = "UTC")
merged_chldata_subset$datetime = format(merged_chldata_subset$datetime,format='%m/%d/%Y %H:%M')

#merge with available data
merged_chldata_subset2 = merge.data.frame(dateseq,merged_chldata_subset,by="datetime",all.x = T)


#replace NAs with linear interpolation 
merged_chldata_subset2[1,2] = 0
merged_chldata_subset2[nrow(merged_chldata_subset2),2] = 0

merged_chldata_subset2$q_cms = na.approx(merged_chldata_subset2$q_cms)


merged_chldata_subset2[1,3] = 0
merged_chldata_subset2[nrow(merged_chldata_subset2),3] = 0

merged_chldata_subset2$conc = na.approx(merged_chldata_subset2$conc)

merged_chldata_subset2 = as.matrix(merged_chldata_subset2) 
colnames(merged_chldata_subset2) = c("datetime","q_cms","conc") ; merged_chldata_subset2 = as.data.frame(merged_chldata_subset2) 
merged_chldata_subset2$q_cms = round(as.numeric(merged_chldata_subset2$q_cms), digits = 4)
merged_chldata_subset2$conc = round(as.numeric(merged_chldata_subset2$conc), digits = 3)







merged_turbdata_subset = merged_data[,c("datetime","meanFlow","turbidity")] ; 
merged_turbdata_subset$meanFlow = merged_turbdata_subset$meanFlow/1000
colnames(merged_turbdata_subset) = c("datetime","q_cms","conc")
merged_turbdata_subset = merged_turbdata_subset[!duplicated(merged_turbdata_subset[,c('datetime')]),] #remove duplicated rows by datetime
merged_turbdata_subset$datetime = lubridate::as_datetime(merged_turbdata_subset$datetime, tz = "UTC")
merged_turbdata_subset$datetime = format(merged_turbdata_subset$datetime,format='%m/%d/%Y %H:%M')

#merge with available data
merged_turbdata_subset2 = merge.data.frame(dateseq,merged_turbdata_subset,by="datetime",all.x = T)

#replace NAs with linear interpolation 
merged_turbdata_subset2[1,2] = 0
merged_turbdata_subset2[nrow(merged_turbdata_subset2),2] = 0

merged_turbdata_subset2$q_cms = na.approx(merged_turbdata_subset2$q_cms)


merged_turbdata_subset2[1,3] = 0
merged_turbdata_subset2[nrow(merged_turbdata_subset2),3] = 0

merged_turbdata_subset2$conc = na.approx(merged_turbdata_subset2$conc)


merged_turbdata_subset2 = as.matrix(merged_turbdata_subset2) 
colnames(merged_turbdata_subset2) = c("datetime","q_cms","conc") ; merged_turbdata_subset2 = as.data.frame(merged_turbdata_subset2) 
merged_turbdata_subset2$q_cms = round(as.numeric(merged_turbdata_subset2$q_cms), digits = 4)
merged_turbdata_subset2$conc = round(as.numeric(merged_turbdata_subset2$conc), digits = 3)

#############
write.csv(merged_chldata_subset2,
          paste("R:/EcosystemEcologyLab/MSAPLANKTONdataDirectory/DataInDevelopment/NEONData/",site,"/03_incremental/merged_chldata_subset_2018.csv",sep="") , row.names=F)

write.csv(merged_turbdata_subset2, 
          paste("R:/EcosystemEcologyLab/MSAPLANKTONdataDirectory/DataInDevelopment/NEONData/",site,"/03_incremental/merged_turbdata_subset_2018.csv",sep="") , 
          row.names=F)




merged_data = read.csv("final_data_30min_2019.csv", header = T)

#bring in datetime vector
dateseq = read.csv("dates_2019.csv", header = T)
dateseq$datetime = lubridate::as_datetime(dateseq$datetime, tz = "UTC")
dateseq$datetime = format(dateseq$datetime,format='%m/%d/%Y %H:%M')

#######################################################################################################
merged_chldata_subset = merged_data[,c("datetime","meanFlow","chlorophyll")] ; 
merged_chldata_subset$meanFlow = merged_chldata_subset$meanFlow/1000
colnames(merged_chldata_subset) = c("datetime","q_cms","conc")
merged_chldata_subset = merged_chldata_subset[!duplicated(merged_chldata_subset[,c('datetime')]),] #remove duplicated rows by datetime
merged_chldata_subset$datetime = lubridate::as_datetime(merged_chldata_subset$datetime, tz = "UTC")
merged_chldata_subset$datetime = format(merged_chldata_subset$datetime,format='%m/%d/%Y %H:%M')

#merge with available data
merged_chldata_subset2 = merge.data.frame(dateseq,merged_chldata_subset,by="datetime",all.x = T)


#replace NAs with linear interpolation 
merged_chldata_subset2[1,2] = 0
merged_chldata_subset2[nrow(merged_chldata_subset2),2] = 0

merged_chldata_subset2$q_cms = na.approx(merged_chldata_subset2$q_cms)


merged_chldata_subset2[1,3] = 0
merged_chldata_subset2[nrow(merged_chldata_subset2),3] = 0

merged_chldata_subset2$conc = na.approx(merged_chldata_subset2$conc)

merged_chldata_subset2 = as.matrix(merged_chldata_subset2) 
colnames(merged_chldata_subset2) = c("datetime","q_cms","conc") ; merged_chldata_subset2 = as.data.frame(merged_chldata_subset2) 
merged_chldata_subset2$q_cms = round(as.numeric(merged_chldata_subset2$q_cms), digits = 4)
merged_chldata_subset2$conc = round(as.numeric(merged_chldata_subset2$conc), digits = 3)







merged_turbdata_subset = merged_data[,c("datetime","meanFlow","turbidity")] ; 
merged_turbdata_subset$meanFlow = merged_turbdata_subset$meanFlow/1000
colnames(merged_turbdata_subset) = c("datetime","q_cms","conc")
merged_turbdata_subset = merged_turbdata_subset[!duplicated(merged_turbdata_subset[,c('datetime')]),] #remove duplicated rows by datetime
merged_turbdata_subset$datetime = lubridate::as_datetime(merged_turbdata_subset$datetime, tz = "UTC")
merged_turbdata_subset$datetime = format(merged_turbdata_subset$datetime,format='%m/%d/%Y %H:%M')

#merge with available data
merged_turbdata_subset2 = merge.data.frame(dateseq,merged_turbdata_subset,by="datetime",all.x = T)

#replace NAs with linear interpolation 
merged_turbdata_subset2[1,2] = 0
merged_turbdata_subset2[nrow(merged_turbdata_subset2),2] = 0

merged_turbdata_subset2$q_cms = na.approx(merged_turbdata_subset2$q_cms)


merged_turbdata_subset2[1,3] = 0
merged_turbdata_subset2[nrow(merged_turbdata_subset2),3] = 0

merged_turbdata_subset2$conc = na.approx(merged_turbdata_subset2$conc)


merged_turbdata_subset2 = as.matrix(merged_turbdata_subset2) 
colnames(merged_turbdata_subset2) = c("datetime","q_cms","conc") ; merged_turbdata_subset2 = as.data.frame(merged_turbdata_subset2) 
merged_turbdata_subset2$q_cms = round(as.numeric(merged_turbdata_subset2$q_cms), digits = 4)
merged_turbdata_subset2$conc = round(as.numeric(merged_turbdata_subset2$conc), digits = 3)

#############
write.csv(merged_chldata_subset2,
          paste("R:/EcosystemEcologyLab/MSAPLANKTONdataDirectory/DataInDevelopment/NEONData/",site,"/03_incremental/merged_chldata_subset_2019.csv",sep="") , row.names=F)

write.csv(merged_turbdata_subset2, 
          paste("R:/EcosystemEcologyLab/MSAPLANKTONdataDirectory/DataInDevelopment/NEONData/",site,"/03_incremental/merged_turbdata_subset_2019.csv",sep="") , 
          row.names=F)




merged_data = read.csv("final_data_30min_2020.csv", header = T)

#bring in datetime vector
dateseq = read.csv("dates_2020.csv", header = T)
dateseq$datetime = lubridate::as_datetime(dateseq$datetime, tz = "UTC")
dateseq$datetime = format(dateseq$datetime,format='%m/%d/%Y %H:%M')

#######################################################################################################
merged_chldata_subset = merged_data[,c("datetime","meanFlow","chlorophyll")] ; 
merged_chldata_subset$meanFlow = merged_chldata_subset$meanFlow/1000
colnames(merged_chldata_subset) = c("datetime","q_cms","conc")
merged_chldata_subset = merged_chldata_subset[!duplicated(merged_chldata_subset[,c('datetime')]),] #remove duplicated rows by datetime
merged_chldata_subset$datetime = lubridate::as_datetime(merged_chldata_subset$datetime, tz = "UTC")
merged_chldata_subset$datetime = format(merged_chldata_subset$datetime,format='%m/%d/%Y %H:%M')

#merge with available data
merged_chldata_subset2 = merge.data.frame(dateseq,merged_chldata_subset,by="datetime",all.x = T)


#replace NAs with linear interpolation 
merged_chldata_subset2[1,2] = 0
merged_chldata_subset2[nrow(merged_chldata_subset2),2] = 0

merged_chldata_subset2$q_cms = na.approx(merged_chldata_subset2$q_cms)


merged_chldata_subset2[1,3] = 0
merged_chldata_subset2[nrow(merged_chldata_subset2),3] = 0

merged_chldata_subset2$conc = na.approx(merged_chldata_subset2$conc)

merged_chldata_subset2 = as.matrix(merged_chldata_subset2) 
colnames(merged_chldata_subset2) = c("datetime","q_cms","conc") ; merged_chldata_subset2 = as.data.frame(merged_chldata_subset2) 
merged_chldata_subset2$q_cms = round(as.numeric(merged_chldata_subset2$q_cms), digits = 4)
merged_chldata_subset2$conc = round(as.numeric(merged_chldata_subset2$conc), digits = 3)







merged_turbdata_subset = merged_data[,c("datetime","meanFlow","turbidity")] ; 
merged_turbdata_subset$meanFlow = merged_turbdata_subset$meanFlow/1000
colnames(merged_turbdata_subset) = c("datetime","q_cms","conc")
merged_turbdata_subset = merged_turbdata_subset[!duplicated(merged_turbdata_subset[,c('datetime')]),] #remove duplicated rows by datetime
merged_turbdata_subset$datetime = lubridate::as_datetime(merged_turbdata_subset$datetime, tz = "UTC")
merged_turbdata_subset$datetime = format(merged_turbdata_subset$datetime,format='%m/%d/%Y %H:%M')

#merge with available data
merged_turbdata_subset2 = merge.data.frame(dateseq,merged_turbdata_subset,by="datetime",all.x = T)

#replace NAs with linear interpolation 
merged_turbdata_subset2[1,2] = 0
merged_turbdata_subset2[nrow(merged_turbdata_subset2),2] = 0

merged_turbdata_subset2$q_cms = na.approx(merged_turbdata_subset2$q_cms)


merged_turbdata_subset2[1,3] = 0
merged_turbdata_subset2[nrow(merged_turbdata_subset2),3] = 0

merged_turbdata_subset2$conc = na.approx(merged_turbdata_subset2$conc)


merged_turbdata_subset2 = as.matrix(merged_turbdata_subset2) 
colnames(merged_turbdata_subset2) = c("datetime","q_cms","conc") ; merged_turbdata_subset2 = as.data.frame(merged_turbdata_subset2) 
merged_turbdata_subset2$q_cms = round(as.numeric(merged_turbdata_subset2$q_cms), digits = 4)
merged_turbdata_subset2$conc = round(as.numeric(merged_turbdata_subset2$conc), digits = 3)

#############
write.csv(merged_chldata_subset2,
          paste("R:/EcosystemEcologyLab/MSAPLANKTONdataDirectory/DataInDevelopment/NEONData/",site,"/03_incremental/merged_chldata_subset_2020.csv",sep="") , row.names=F)

write.csv(merged_turbdata_subset2, 
          paste("R:/EcosystemEcologyLab/MSAPLANKTONdataDirectory/DataInDevelopment/NEONData/",site,"/03_incremental/merged_turbdata_subset_2020.csv",sep="") , 
          row.names=F)




merged_data = read.csv("final_data_30min_2021.csv", header = T)

#bring in datetime vector
dateseq = read.csv("dates_2021.csv", header = T)
dateseq$datetime = lubridate::as_datetime(dateseq$datetime, tz = "UTC")
dateseq$datetime = format(dateseq$datetime,format='%m/%d/%Y %H:%M')

######################################################################################################
merged_chldata_subset = merged_data[,c("datetime","meanFlow","chlorophyll")] ; 
merged_chldata_subset$meanFlow = merged_chldata_subset$meanFlow/1000
colnames(merged_chldata_subset) = c("datetime","q_cms","conc")
merged_chldata_subset = merged_chldata_subset[!duplicated(merged_chldata_subset[,c('datetime')]),] #remove duplicated rows by datetime
merged_chldata_subset$datetime = lubridate::as_datetime(merged_chldata_subset$datetime, tz = "UTC")
merged_chldata_subset$datetime = format(merged_chldata_subset$datetime,format='%m/%d/%Y %H:%M')

#merge with available data
merged_chldata_subset2 = merge.data.frame(dateseq,merged_chldata_subset,by="datetime",all.x = T)


#replace NAs with linear interpolation 
merged_chldata_subset2[1,2] = 0
merged_chldata_subset2[nrow(merged_chldata_subset2),2] = 0

merged_chldata_subset2$q_cms = na.approx(merged_chldata_subset2$q_cms)


merged_chldata_subset2[1,3] = 0
merged_chldata_subset2[nrow(merged_chldata_subset2),3] = 0

merged_chldata_subset2$conc = na.approx(merged_chldata_subset2$conc)

merged_chldata_subset2 = as.matrix(merged_chldata_subset2) 
colnames(merged_chldata_subset2) = c("datetime","q_cms","conc") ; merged_chldata_subset2 = as.data.frame(merged_chldata_subset2) 
merged_chldata_subset2$q_cms = round(as.numeric(merged_chldata_subset2$q_cms), digits = 4)
merged_chldata_subset2$conc = round(as.numeric(merged_chldata_subset2$conc), digits = 3)







merged_turbdata_subset = merged_data[,c("datetime","meanFlow","turbidity")] ; 
merged_turbdata_subset$meanFlow = merged_turbdata_subset$meanFlow/1000
colnames(merged_turbdata_subset) = c("datetime","q_cms","conc")
merged_turbdata_subset = merged_turbdata_subset[!duplicated(merged_turbdata_subset[,c('datetime')]),] #remove duplicated rows by datetime
merged_turbdata_subset$datetime = lubridate::as_datetime(merged_turbdata_subset$datetime, tz = "UTC")
merged_turbdata_subset$datetime = format(merged_turbdata_subset$datetime,format='%m/%d/%Y %H:%M')

#merge with available data
merged_turbdata_subset2 = merge.data.frame(dateseq,merged_turbdata_subset,by="datetime",all.x = T)

#replace NAs with linear interpolation 
merged_turbdata_subset2[1,2] = 0
merged_turbdata_subset2[nrow(merged_turbdata_subset2),2] = 0

merged_turbdata_subset2$q_cms = na.approx(merged_turbdata_subset2$q_cms)


merged_turbdata_subset2[1,3] = 0
merged_turbdata_subset2[nrow(merged_turbdata_subset2),3] = 0

merged_turbdata_subset2$conc = na.approx(merged_turbdata_subset2$conc)


merged_turbdata_subset2 = as.matrix(merged_turbdata_subset2) 
colnames(merged_turbdata_subset2) = c("datetime","q_cms","conc") ; merged_turbdata_subset2 = as.data.frame(merged_turbdata_subset2) 
merged_turbdata_subset2$q_cms = round(as.numeric(merged_turbdata_subset2$q_cms), digits = 4)
merged_turbdata_subset2$conc = round(as.numeric(merged_turbdata_subset2$conc), digits = 3)

#############
write.csv(merged_chldata_subset2,
          paste("R:/EcosystemEcologyLab/MSAPLANKTONdataDirectory/DataInDevelopment/NEONData/",site,"/03_incremental/merged_chldata_subset_2021.csv",sep="") , row.names=F)

write.csv(merged_turbdata_subset2, 
          paste("R:/EcosystemEcologyLab/MSAPLANKTONdataDirectory/DataInDevelopment/NEONData/",site,"/03_incremental/merged_turbdata_subset_2021.csv",sep="") , 
          row.names=F)




merged_data = read.csv("final_data_30min_2022.csv", header = T)

#bring in datetime vector
dateseq = read.csv("dates_2022.csv", header = T)
dateseq$datetime = lubridate::as_datetime(dateseq$datetime, tz = "UTC")
dateseq$datetime = format(dateseq$datetime,format='%m/%d/%Y %H:%M')

######################################################################################################
merged_chldata_subset = merged_data[,c("datetime","meanFlow","chlorophyll")] ; 
merged_chldata_subset$meanFlow = merged_chldata_subset$meanFlow/1000
colnames(merged_chldata_subset) = c("datetime","q_cms","conc")
merged_chldata_subset = merged_chldata_subset[!duplicated(merged_chldata_subset[,c('datetime')]),] #remove duplicated rows by datetime
merged_chldata_subset$datetime = lubridate::as_datetime(merged_chldata_subset$datetime, tz = "UTC")
merged_chldata_subset$datetime = format(merged_chldata_subset$datetime,format='%m/%d/%Y %H:%M')

#merge with available data
merged_chldata_subset2 = merge.data.frame(dateseq,merged_chldata_subset,by="datetime",all.x = T)


#replace NAs with linear interpolation 
merged_chldata_subset2[1,2] = 0
merged_chldata_subset2[nrow(merged_chldata_subset2),2] = 0

merged_chldata_subset2$q_cms = na.approx(merged_chldata_subset2$q_cms)


merged_chldata_subset2[1,3] = 0
merged_chldata_subset2[nrow(merged_chldata_subset2),3] = 0

merged_chldata_subset2$conc = na.approx(merged_chldata_subset2$conc)

merged_chldata_subset2 = as.matrix(merged_chldata_subset2) 
colnames(merged_chldata_subset2) = c("datetime","q_cms","conc") ; merged_chldata_subset2 = as.data.frame(merged_chldata_subset2) 
merged_chldata_subset2$q_cms = round(as.numeric(merged_chldata_subset2$q_cms), digits = 4)
merged_chldata_subset2$conc = round(as.numeric(merged_chldata_subset2$conc), digits = 3)







merged_turbdata_subset = merged_data[,c("datetime","meanFlow","turbidity")] ; 
merged_turbdata_subset$meanFlow = merged_turbdata_subset$meanFlow/1000
colnames(merged_turbdata_subset) = c("datetime","q_cms","conc")
merged_turbdata_subset = merged_turbdata_subset[!duplicated(merged_turbdata_subset[,c('datetime')]),] #remove duplicated rows by datetime
merged_turbdata_subset$datetime = lubridate::as_datetime(merged_turbdata_subset$datetime, tz = "UTC")
merged_turbdata_subset$datetime = format(merged_turbdata_subset$datetime,format='%m/%d/%Y %H:%M')

#merge with available data
merged_turbdata_subset2 = merge.data.frame(dateseq,merged_turbdata_subset,by="datetime",all.x = T)

#replace NAs with linear interpolation 
merged_turbdata_subset2[1,2] = 0
merged_turbdata_subset2[nrow(merged_turbdata_subset2),2] = 0

merged_turbdata_subset2$q_cms = na.approx(merged_turbdata_subset2$q_cms)


merged_turbdata_subset2[1,3] = 0
merged_turbdata_subset2[nrow(merged_turbdata_subset2),3] = 0

merged_turbdata_subset2$conc = na.approx(merged_turbdata_subset2$conc)


merged_turbdata_subset2 = as.matrix(merged_turbdata_subset2) 
colnames(merged_turbdata_subset2) = c("datetime","q_cms","conc") ; merged_turbdata_subset2 = as.data.frame(merged_turbdata_subset2) 
merged_turbdata_subset2$q_cms = round(as.numeric(merged_turbdata_subset2$q_cms), digits = 4)
merged_turbdata_subset2$conc = round(as.numeric(merged_turbdata_subset2$conc), digits = 3)

#############
write.csv(merged_chldata_subset2,
          paste("R:/EcosystemEcologyLab/MSAPLANKTONdataDirectory/DataInDevelopment/NEONData/",site,"/03_incremental/merged_chldata_subset_2022.csv",sep="") , row.names=F)

write.csv(merged_turbdata_subset2, 
          paste("R:/EcosystemEcologyLab/MSAPLANKTONdataDirectory/DataInDevelopment/NEONData/",site,"/03_incremental/merged_turbdata_subset_2022.csv",sep="") , 
          row.names=F)



```

cQ analysis for chlorophyll/year/site following (Millar et al. 2021)
```{r setup, include=FALSE}
library(tidyverse) ; library(viridis)

site="ARIK"
year="2017"

# Vector containing candidate baseflow separation filter values
candidateFilterPara <- c(0.98)
# Vector containing candidate stormflow threshold values
candidateSfThresh <- c(1)

###################
# SET DIRECTORIES #
###################

# Define the input directory, functions, and input files
input_dir <- paste("R:/EcosystemEcologyLab/MSAPLANKTONdataDirectory/DataInDevelopment/NEONData/",site,"/03_incremental",sep="")
output_dir <- paste("R:/EcosystemEcologyLab/MSAPLANKTONdataDirectory/DataInDevelopment/NEONData/",site,"/04_product/Chlorophyll_",year,sep="")
###################### READ IN FUNCTIONS #
source(file.path("R:/EcosystemEcologyLab/MSAPLANKTONdataDirectory/DataInDevelopment/NEONData/NEONdataScript/cQ_analysis/cQ_functions.R"))

################
# READ IN DATA #
################
################RUN THE CODE############
# Read in 30-min discharge data
allInputData30Min <- read.csv(file.path(input_dir,paste("merged_chldata_subset_",year,".csv",sep="")))

# Specify constituent in data set name
dataSetName <- "chl"

# Chose constitution for plot axes labels (NO3, TOC, or turbidity)
constit <- "Chl"

allInputData30Min$datetime <- as.POSIXct(allInputData30Min$datetime,format("%m/%d/%Y %H:%M"),tz="UTC")

allInputData30Min <- allInputData30Min %>% 
  dplyr::mutate(rescaled_conc = ((conc-min(conc))/(max(conc)-min(conc))*max(q_cms)))



# Vector with interpolation intervals used for calculating HI
interp <- seq(0,1,0.01)

##########################################
# RUN ANALYSIS TO GET HYSTERESIS INDICES #
##########################################

batchRun1 <- batchRunBfAndEvSepForCQ(qInputs = allInputData30Min,
                                     bfSepPasses = 3,
                                     filterParam = candidateFilterPara,
                                     sfSmoothPasses = 4,
                                     sfThresh = candidateSfThresh,
                                     cInputs = allInputData30Min,
                                     timeStep = 30,
                                     minDuration = 18,
                                     maxDuration = 200)

eventsDataAll1 <- getAllStormEvents(batchRun = batchRun1,
                                    timestep_min = 30)

batchRunFlowsLF1 <- batchRunflowCompare(qData = allInputData30Min,
                                         bfSepPasses = 4,
                                         filterPara = candidateFilterPara,
                                         sfSmoothPasses = 4)

eventsData1 <- stormEventCalcs(batchRun = batchRun1,
                               timestep_min = 30)

stormCounts1 <- stormCounts(batchRun1)

hysteresisData1 <- getHysteresisIndices(batchRun = batchRun1,
                                        xForInterp = interp,
                                        eventsData = eventsData1)

######################
# EXPORT OUTPUT DATA #
######################

write.csv(eventsData1,file = file.path(output_dir,paste(dataSetName,"_StormEventSummaryData.csv",sep="")))
write.csv(batchRunFlowsLF1,file = file.path(output_dir,paste(dataSetName,"_DischargeData.csv",sep="")))
write.csv(hysteresisData1,file = file.path(output_dir,paste(dataSetName,"_HysteresisData.csv",sep="")))
write.csv(eventsDataAll1,file = file.path(output_dir,paste(dataSetName,"_AllCQData.csv",sep="")))
write.csv(stormCounts1,file = file.path(output_dir,paste(dataSetName,"_StormCounts.csv",sep="")))


#########################################
# PLOT AND SAVE DATA - EVENT SEPARATION #
#########################################

# Make subfolder in output directory to save hydrograph plots
dir.create(file.path(output_dir, "Hydrographs"), showWarnings = FALSE)

# 1) Plot and save the hydrograph with input data

initialHydrograph <- ggplot(allInputData30Min,aes(x=datetime, y=q_cms)) +
                            geom_line(size=0.5, color="black") +
                            xlab(NULL) +
                            ylab(expression(paste("Total discharge (",m^3," ",s^-1,")"))) +
                            theme_bw() +
                            theme(text=element_text(size=18))

ggsave(file=file.path(output_dir,"Hydrographs",paste(dataSetName,"_TotalDischarge.jpeg")),
       initialHydrograph,
       width = 12, 
       height = 4, 
       units = "in",
       dpi=600)


# 2) Plot total discharge with baseflow

baseflowHydrograph <- ggplot() + 
                            geom_line(data=batchRunFlowsLF1, aes(x=datetime, y=total_flow), size=0.5, color="black") +
                            geom_line(data=batchRunFlowsLF1, aes(x=datetime, y=base_flow,color=filter_para), size=0.75) +
                            scale_color_brewer(palette = "Set1") +
                            xlab(NULL) +
                            ylab(expression(paste("Discharge (",m^3," ",s^-1,")"))) +
                            theme_bw() +
                            theme(legend.title = element_blank(),
                                  text=element_text(size=18))

ggsave(file=file.path(output_dir,"Hydrographs",paste(dataSetName,"_Baseflows.jpeg")),
       baseflowHydrograph,
       width = 14, 
       height = 4, 
       units = "in",
       dpi=600)


# 3) Plot smoothed storm flows

stormflowHydrograph <- ggplot() + 
  geom_line(data=batchRunFlowsLF1, aes(x=datetime, y=storm_flow,color=filter_para), size=0.75) +
  scale_color_brewer(palette = "Set1") +
  xlab(NULL) +
  ylab(expression(paste("Storm flow (",m^3," ",s^-1,")"))) +
  theme_bw() +
  theme(legend.title = element_blank(),
        text=element_text(size=18))

ggsave(file=file.path(output_dir,"Hydrographs",paste(dataSetName,"_StormflowsOnly.jpeg")),
       stormflowHydrograph,
       width = 14, 
       height = 4, 
       units = "in",
       dpi=600)


# 3a) Plot smoothed storm flows with storm flow thresholds

stormflowThreshHydrograph <- ggplot() + 
  geom_line(data=batchRunFlowsLF1, aes(x=datetime, y=storm_flow,color=filter_para), size=0.75) +
  scale_color_brewer(palette = "Set1") +
  geom_hline(yintercept = candidateSfThresh, linetype = "dashed", color = "black",alpha=0.5) +
  xlab(NULL) +
  ylab(expression(paste("Storm flow (",m^3," ",s^-1,")"))) +
  theme_bw() +
  theme(legend.title = element_blank(),
        text=element_text(size=18))

ggsave(file=file.path(output_dir,"Hydrographs",paste(dataSetName,"_StormflowsOnlyWithThresholds.jpeg")),
       stormflowThreshHydrograph,
       width = 14, 
       height = 4, 
       units = "in",
       dpi=600)


# 4) Plot batch run event separation hydrographs
eventsDataShaded1 <- eventsData1 %>% mutate(start = as.POSIXct(start,
                                                                   format("%Y-%m-%d %H:%M:%S"),tz="EST"),
                                            end = as.POSIXct(end,
                                                                   format("%Y-%m-%d %H:%M:%S"),tz="EST"),
                                            tops = max(allInputData30Min$q_cms),
                                            bottoms = 0)

batchEventSepPlot <- ggplot() + 
  geom_rect(data=eventsDataShaded1, mapping=aes(xmin=start, 
                                                xmax=end, 
                                                ymin=bottoms, 
                                                ymax=tops), fill="green", color="red", alpha=0.2) +
  
  geom_line(data=allInputData30Min, aes(x=datetime, y=q_cms), size=0.8, color="blue") +
  geom_line(data=allInputData30Min, aes(x=datetime, y=rescaled_conc), size=0.5, color="black",linetype="dashed") +
  facet_wrap(~ run_id, ncol = 1) +
  scale_color_brewer(palette = "Set1") +
  xlab(NULL) +
  ylab(expression(paste("Discharge (",m^3," ",s^-1,")"))) +
  theme_bw() +
  theme(legend.title = element_blank(),
        text=element_text(size=18))


ggsave(file=file.path(output_dir,"Hydrographs",paste(dataSetName,"_BatchEventSeparationPlot.jpeg")),
       batchEventSepPlot,
       width = 8, 
       height = 10, 
       units = "in",
       dpi=600)

####################################
# PLOT AND SAVE DATA - c-Q RESULTS #
####################################

if (constit == "chl") {
  
  makeCQPlotsNO3(batchRun1)
  makeHystFlushPlotsNO3(hysteresisData1)
  
} else if (constit == "turb") {

makeCQPlotsTOC(batchRun1)
makeHystFlushPlotsTOC(hysteresisData1)
  
} else if (constit == "turbity") {
    
makeCQPlotsTurb(batchRun1) 
makeHystFlushPlotsTurb(hysteresisData1)
  
}

#chlorophyll
```

cQ analysis for turbidity/year/site following (Millar et al. 2021)
```{r setup, include=FALSE}
library(tidyverse) ; library(viridis)

# Main script to separate baseflow, delineate storm events, 
# and run c-Q hysteresis index analyses

# Clear memory
rm(list=ls())

site="ARIK"
year="2017"

# Vector containing candidate baseflow separation filter values
candidateFilterPara <- c(0.98)
# Vector containing candidate stormflow threshold values
candidateSfThresh <- c(1)

###################
# SET DIRECTORIES #
###################

# Define the input directory, functions, and input files
input_dir <- paste("R:/EcosystemEcologyLab/MSAPLANKTONdataDirectory/DataInDevelopment/NEONData/",site,"/03_incremental",sep="")
output_dir <- paste("R:/EcosystemEcologyLab/MSAPLANKTONdataDirectory/DataInDevelopment/NEONData/",site,"/04_product/Turbidity_",year,sep="")##################### READ IN FUNCTIONS #
source(file.path("R:/EcosystemEcologyLab/MSAPLANKTONdataDirectory/DataInDevelopment/NEONData/NEONdataScript/cQ_analysis/cQ_functions.R"))

################
# READ IN DATA #
################

# Read in 30-min discharge data
allInputData30Min <- read.csv(file.path(input_dir,paste("merged_turbdata_subset_",year,".csv",sep="")))

################RUN THE CODE############
# Specify constituent in data set name
dataSetName <- "turb"

# Chose constitution for plot axes labels (NO3, TOC, or turbidity)
constit <- "Turb"

allInputData30Min$datetime <- as.POSIXct(allInputData30Min$datetime,format("%m/%d/%Y %H:%M"),tz="UTC")

allInputData30Min <- allInputData30Min %>% 
  mutate(rescaled_conc = ((conc-min(conc))/(max(conc)-min(conc))*max(q_cms)))


# Vector with interpolation intervals used for calculating HI
interp <- seq(0,1,0.01)

##########################################
# RUN ANALYSIS TO GET HYSTERESIS INDICES #
##########################################

batchRun1 <- batchRunBfAndEvSepForCQ(qInputs = allInputData30Min,
                                     bfSepPasses = 3,
                                     filterParam = candidateFilterPara,
                                     sfSmoothPasses = 4,
                                     sfThresh = candidateSfThresh,
                                     cInputs = allInputData30Min,
                                     timeStep = 30,
                                     minDuration = 18,
                                     maxDuration = 200)

eventsDataAll1 <- getAllStormEvents(batchRun = batchRun1,
                                    timestep_min = 30)

batchRunFlowsLF1 <- batchRunflowCompare(qData = allInputData30Min,
                                         bfSepPasses = 4,
                                         filterPara = candidateFilterPara,
                                         sfSmoothPasses = 4)

eventsData1 <- stormEventCalcs(batchRun = batchRun1,
                               timestep_min = 30)

stormCounts1 <- stormCounts(batchRun1)

hysteresisData1 <- getHysteresisIndices(batchRun = batchRun1,
                                        xForInterp = interp,
                                        eventsData = eventsData1)

######################
# EXPORT OUTPUT DATA #
######################

write.csv(eventsData1,file = file.path(output_dir,paste(dataSetName,"_StormEventSummaryData.csv",sep="")))
write.csv(batchRunFlowsLF1,file = file.path(output_dir,paste(dataSetName,"_DischargeData.csv",sep="")))
write.csv(hysteresisData1,file = file.path(output_dir,paste(dataSetName,"_HysteresisData.csv",sep="")))
write.csv(eventsDataAll1,file = file.path(output_dir,paste(dataSetName,"_AllCQData.csv",sep="")))
write.csv(stormCounts1,file = file.path(output_dir,paste(dataSetName,"_StormCounts.csv",sep="")))


#########################################
# PLOT AND SAVE DATA - EVENT SEPARATION #
#########################################

# Make subfolder in output directory to save hydrograph plots
dir.create(file.path(output_dir, "Hydrographs"), showWarnings = FALSE)

# 1) Plot and save the hydrograph with input data

initialHydrograph <- ggplot(allInputData30Min,aes(x=datetime, y=q_cms)) +
                            geom_line(size=0.5, color="black") +
                            xlab(NULL) +
                            ylab(expression(paste("Total discharge (",m^3," ",s^-1,")"))) +
                            theme_bw() +
                            theme(text=element_text(size=18))

ggsave(file=file.path(output_dir,"Hydrographs",paste(dataSetName,"_TotalDischarge.jpeg")),
       initialHydrograph,
       width = 12, 
       height = 4, 
       units = "in",
       dpi=600)


# 2) Plot total discharge with baseflow

baseflowHydrograph <- ggplot() + 
                            geom_line(data=batchRunFlowsLF1, aes(x=datetime, y=total_flow), size=0.5, color="black") +
                            geom_line(data=batchRunFlowsLF1, aes(x=datetime, y=base_flow,color=filter_para), size=0.75) +
                            scale_color_brewer(palette = "Set1") +
                            xlab(NULL) +
                            ylab(expression(paste("Discharge (",m^3," ",s^-1,")"))) +
                            theme_bw() +
                            theme(legend.title = element_blank(),
                                  text=element_text(size=18))

ggsave(file=file.path(output_dir,"Hydrographs",paste(dataSetName,"_Baseflows.jpeg")),
       baseflowHydrograph,
       width = 14, 
       height = 4, 
       units = "in",
       dpi=600)


# 3) Plot smoothed storm flows

stormflowHydrograph <- ggplot() + 
  geom_line(data=batchRunFlowsLF1, aes(x=datetime, y=storm_flow,color=filter_para), size=0.75) +
  scale_color_brewer(palette = "Set1") +
  xlab(NULL) +
  ylab(expression(paste("Storm flow (",m^3," ",s^-1,")"))) +
  theme_bw() +
  theme(legend.title = element_blank(),
        text=element_text(size=18))

ggsave(file=file.path(output_dir,"Hydrographs",paste(dataSetName,"_StormflowsOnly.jpeg")),
       stormflowHydrograph,
       width = 14, 
       height = 4, 
       units = "in",
       dpi=600)


# 3a) Plot smoothed storm flows with storm flow thresholds

stormflowThreshHydrograph <- ggplot() + 
  geom_line(data=batchRunFlowsLF1, aes(x=datetime, y=storm_flow,color=filter_para), size=0.75) +
  scale_color_brewer(palette = "Set1") +
  geom_hline(yintercept = candidateSfThresh, linetype = "dashed", color = "black",alpha=0.5) +
  xlab(NULL) +
  ylab(expression(paste("Storm flow (",m^3," ",s^-1,")"))) +
  theme_bw() +
  theme(legend.title = element_blank(),
        text=element_text(size=18))

ggsave(file=file.path(output_dir,"Hydrographs",paste(dataSetName,"_StormflowsOnlyWithThresholds.jpeg")),
       stormflowThreshHydrograph,
       width = 14, 
       height = 4, 
       units = "in",
       dpi=600)


# 4) Plot batch run event separation hydrographs
eventsDataShaded1 <- eventsData1 %>% mutate(start = as.POSIXct(start,
                                                                   format("%Y-%m-%d %H:%M:%S"),tz="EST"),
                                            end = as.POSIXct(end,
                                                                   format("%Y-%m-%d %H:%M:%S"),tz="EST"),
                                            tops = max(allInputData30Min$q_cms),
                                            bottoms = 0)

batchEventSepPlot <- ggplot() + 
  geom_rect(data=eventsDataShaded1, mapping=aes(xmin=start, 
                                                xmax=end, 
                                                ymin=bottoms, 
                                                ymax=tops), fill="green", color="red", alpha=0.2) +
  
  geom_line(data=allInputData30Min, aes(x=datetime, y=q_cms), size=0.8, color="blue") +
  geom_line(data=allInputData30Min, aes(x=datetime, y=rescaled_conc), size=0.5, color="black",linetype="dashed") +
  facet_wrap(~ run_id, ncol = 1) +
  scale_color_brewer(palette = "Set1") +
  xlab(NULL) +
  ylab(expression(paste("Discharge (",m^3," ",s^-1,")"))) +
  theme_bw() +
  theme(legend.title = element_blank(),
        text=element_text(size=18))


ggsave(file=file.path(output_dir,"Hydrographs",paste(dataSetName,"_BatchEventSeparationPlot.jpeg")),
       batchEventSepPlot,
       width = 8, 
       height = 10, 
       units = "in",
       dpi=600)

####################################
# PLOT AND SAVE DATA - c-Q RESULTS #
####################################

if (constit == "chl") {
  
  makeCQPlotsNO3(batchRun1)
  makeHystFlushPlotsNO3(hysteresisData1)
  
} else if (constit == "turb") {

makeCQPlotsTOC(batchRun1)
makeHystFlushPlotsTOC(hysteresisData1)
  
} else if (constit == "turbity") {
    
makeCQPlotsTurb(batchRun1) 
makeHystFlushPlotsTurb(hysteresisData1)
  
}


#Turbidity
```